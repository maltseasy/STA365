{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "## Stochastic Processes\n",
    "\n",
    "A stochastic process $\\{ X_t : t \\in T\\}$ is a collection of random variables indexed by some set T. \n",
    "\n",
    "## Gaussian Processes\n",
    "\n",
    "A kind of stochastic process where every finite colleciton of random variables $(X_{t_1}, X_{t_2}, ..., X_{t_n})$ has a multivariate normal distribution. \n",
    "\n",
    "Formally, a GP $\\{f(x)\\}$ is specified by: \n",
    "- A mean function $m(x) = \\mathbb{E} [f(x)]$.\n",
    "- A covariance function or kernel $k(x,x') = Cov[f(x),f(x')]$.\n",
    "\n",
    "We write: $f(x) \\sim GP(m(x), k(x,x'))$.\n",
    "\n",
    "This is nice because now we can interpret the kernel as capturing how outputs x and x' co-vary. By choosing different kernels, we embed different assumptions about the function's smoothness, periodicity, etc. \n",
    "\n",
    "## Variational Inference\n",
    "This is a method for approximating difficult to compute posterior distributions $p(\\theta | x)$ in a Bayesian model. Instead of sampling like in MCMC, VI uses optimization. \n",
    "- We posit a simple family of distributions $Q$ that we will use as a stand-in for the complex true posterior. \n",
    "- We then find the member of this simple family $q*(\\theta)$ that is closest (in terms of KL divergence) to the true posterior $p(\\theta | x)$.\n",
    "\n",
    "## Evidence Lower Bound (Elbo)\n",
    "\n",
    "For a proposed variational distribution $q(\\theta)$, $ELBO(q) = \\mathbb{E}_{q(\\theta)}[log(p(x,\\theta)) - log(q(\\theta))]$\n",
    "- The VI procedure is to maximize $ELBO(q)$ over all $q \\in Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data = fetch_california_housing()\n",
    "X = cal_data.data\n",
    "y = cal_data.target\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_small = 200\n",
    "X_train_small = X_train_full[:n_small]\n",
    "y_train_small = y_train_full[:n_small]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_small_scaled = scaler.fit_transform(X_train_small)\n",
    "X_test_scaled = scaler.transform(X_test_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanmisra/miniforge3/envs/pymc_env/lib/python3.12/site-packages/pymc/gp/gp.py:55: FutureWarning: The 'noise' parameter has been been changed to 'sigma' in order to standardize the GP API and will be deprecated in future releases.\n",
      "  warnings.warn(_noise_deprecation_warning, FutureWarning)\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [len_scale, eta, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a1edb50ba440ff91b65b8463b13dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 179 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "len_scale  5.838  1.257   3.741    8.297      0.030    0.021    1716.0   \n",
      "eta        2.966  0.689   1.820    4.249      0.016    0.012    1759.0   \n",
      "sigma      0.592  0.036   0.525    0.660      0.001    0.001    2007.0   \n",
      "\n",
      "           ess_tail  r_hat  \n",
      "len_scale    2432.0    1.0  \n",
      "eta          2309.0    1.0  \n",
      "sigma        2121.0    1.0  \n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as gp_model:\n",
    "    len_scale = pm.Gamma('len_scale', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "\n",
    "    #rbf kernel\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(input_dim=X_train_small.shape[1], ls=len_scale)\n",
    "    mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=cov_func)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=2)\n",
    "\n",
    "    y_obs = gp.marginal_likelihood(\"y_obs\", X_train_small_scaled, y_train_small, noise=sigma)\n",
    "    trace_gp = pm.sample(1000, tune=1000, \n",
    "                         chains=4, cores=1, target_accept=0.95)\n",
    "    \n",
    "print(az.summary(trace_gp, var_names=['len_scale', 'eta', 'sigma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Test MSE (trained on 200 pts): 0.4520293868467607\n"
     ]
    }
   ],
   "source": [
    "post_means = trace_gp.posterior.mean(dim=[\"chain\", \"draw\"])\n",
    "point_dict = {\n",
    "    \"len_scale\": float(post_means[\"len_scale\"].values),\n",
    "    \"eta\": float(post_means[\"eta\"].values),\n",
    "    \"sigma\": float(post_means[\"sigma\"].values),\n",
    "}\n",
    "with gp_model:\n",
    "    mu_pred, var_pred = gp.predict(\n",
    "        X_test_scaled, point=point_dict,\n",
    "        diag=True, pred_noise=True\n",
    "    )\n",
    "\n",
    "# 8) Evaluate MSE\n",
    "mse_gp = np.mean((mu_pred - y_test_full)**2)\n",
    "print(\"GP Test MSE (trained on 200 pts):\", mse_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964df12c22664333b087326739a319ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 520.76\n",
      "arviz - WARNING - Shape validation failed: input_shape: (1, 2000), minimum_shape: (chains=2, draws=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "W1[0, 0]  0.021  0.887  -1.467    1.875      0.020    0.015    2015.0   \n",
      "W1[0, 1]  0.044  0.888  -1.659    1.649      0.020    0.014    1963.0   \n",
      "W1[0, 2]  0.115  0.833  -1.322    1.822      0.018    0.013    2087.0   \n",
      "W1[0, 3]  0.076  0.898  -1.585    1.791      0.020    0.014    1949.0   \n",
      "W1[0, 4]  0.030  0.876  -1.599    1.657      0.020    0.015    1897.0   \n",
      "...         ...    ...     ...      ...        ...      ...       ...   \n",
      "W2[47]   -0.024  0.380  -0.733    0.701      0.009    0.006    1922.0   \n",
      "W2[48]    0.044  0.379  -0.680    0.745      0.009    0.006    1907.0   \n",
      "W2[49]    0.014  0.395  -0.721    0.754      0.009    0.006    2039.0   \n",
      "b2        1.778  0.356   1.126    2.451      0.008    0.006    1933.0   \n",
      "sigma     2.683  0.418   1.947    3.493      0.010    0.007    1860.0   \n",
      "\n",
      "          ess_tail  r_hat  \n",
      "W1[0, 0]    1841.0    NaN  \n",
      "W1[0, 1]    1912.0    NaN  \n",
      "W1[0, 2]    1879.0    NaN  \n",
      "W1[0, 3]    1889.0    NaN  \n",
      "W1[0, 4]    1688.0    NaN  \n",
      "...            ...    ...  \n",
      "W2[47]      1880.0    NaN  \n",
      "W2[48]      1822.0    NaN  \n",
      "W2[49]      1961.0    NaN  \n",
      "b2          1856.0    NaN  \n",
      "sigma       1812.0    NaN  \n",
      "\n",
      "[502 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 50\n",
    "n_features = X_train_small.shape[1]\n",
    "with pm.Model() as bnn_small:\n",
    "    W1 = pm.Normal('W1', mu=0, sigma=1, shape=(n_features, n_hidden))\n",
    "    b1 = pm.Normal('b1', mu=0, sigma=1, shape=n_hidden)\n",
    "\n",
    "    layer1 = pm.math.tanh(pm.math.dot(X_train_small_scaled, W1) + b1)\n",
    "\n",
    "    W2 = pm.Normal('W2', mu=0, sigma=1, shape=(n_hidden,))\n",
    "    b2 = pm.Normal('b2', mu=0, sigma=1)\n",
    "\n",
    "    mu = pm.math.dot(layer1, W2) + b2\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y_train_small)\n",
    "\n",
    "    approx = pm.fit(n=12000, method=\"advi\")\n",
    "    trace_bnn_small = approx.sample(draws=2000)\n",
    "print(az.summary(trace_bnn_small, var_names=['W1', 'b1', 'W2', 'b2', 'sigma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small BNN Test MSE (trained on 200 pts): 1.138061048540759\n"
     ]
    }
   ],
   "source": [
    "posterior_means_bnn_small = trace_bnn_small.posterior.mean(dim=[\"chain\", \"draw\"])\n",
    "\n",
    "W1_m = posterior_means_bnn_small['W1'].values\n",
    "b1_m = posterior_means_bnn_small['b1'].values\n",
    "W2_m = posterior_means_bnn_small['W2'].values\n",
    "b2_m = float(posterior_means_bnn_small['b2'].values)\n",
    "\n",
    "def nn_predict(X_):\n",
    "    layer1_ = np.tanh(X_ @ W1_m + b1_m)\n",
    "    return layer1_ @ W2_m + b2_m\n",
    "\n",
    "y_pred_small_bnn = nn_predict(X_test_scaled)\n",
    "mse_small_bnn = np.mean((y_pred_small_bnn - y_test_full)**2)\n",
    "print(\"Small BNN Test MSE (trained on 200 pts):\", mse_small_bnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7040c216bfbf407bb1d878c31de77a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 27,513\n",
      "arviz - WARNING - Shape validation failed: input_shape: (1, 2000), minimum_shape: (chains=2, draws=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "W1[0, 0] -0.082  0.656  -1.307    1.129      0.015    0.013    1789.0   \n",
      "W1[0, 1]  0.018  0.681  -1.298    1.267      0.015    0.011    2127.0   \n",
      "W1[0, 2]  0.030  0.669  -1.173    1.280      0.015    0.011    1942.0   \n",
      "W1[0, 3]  0.076  0.725  -1.278    1.435      0.016    0.011    1995.0   \n",
      "W1[0, 4]  0.013  0.649  -1.254    1.177      0.014    0.010    2141.0   \n",
      "...         ...    ...     ...      ...        ...      ...       ...   \n",
      "W3[17]    0.013  0.131  -0.243    0.255      0.003    0.002    2023.0   \n",
      "W3[18]   -0.017  0.140  -0.273    0.242      0.003    0.002    1669.0   \n",
      "W3[19]   -0.001  0.133  -0.258    0.240      0.003    0.002    2034.0   \n",
      "b3        2.068  0.109   1.886    2.297      0.003    0.002    1727.0   \n",
      "sigma     1.268  0.061   1.154    1.385      0.001    0.001    1919.0   \n",
      "\n",
      "          ess_tail  r_hat  \n",
      "W1[0, 0]    1377.0    NaN  \n",
      "W1[0, 1]    2011.0    NaN  \n",
      "W1[0, 2]    1816.0    NaN  \n",
      "W1[0, 3]    1887.0    NaN  \n",
      "W1[0, 4]    1911.0    NaN  \n",
      "...            ...    ...  \n",
      "W3[17]      1958.0    NaN  \n",
      "W3[18]      1961.0    NaN  \n",
      "W3[19]      1982.0    NaN  \n",
      "b3          1803.0    NaN  \n",
      "sigma       1713.0    NaN  \n",
      "\n",
      "[622 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler_full = StandardScaler()\n",
    "X_train_full_scaled = scaler_full.fit_transform(X_train_full)\n",
    "X_test_full_scaled = scaler_full.transform(X_test_full)\n",
    "\n",
    "n_hidden_1 = 20\n",
    "n_hidden_2 = 20\n",
    "\n",
    "with pm.Model() as bnn_large:\n",
    "    W1 = pm.Normal('W1', mu=0, sigma=1, shape=(n_features, n_hidden_1))\n",
    "    b1 = pm.Normal('b1', mu=0, sigma=1, shape=(n_hidden_1,))\n",
    "\n",
    "    layer1 = pm.math.tanh(pm.math.dot(X_train_full_scaled, W1) + b1)\n",
    "\n",
    "    W2 = pm.Normal('W2', mu=0, sigma=1, shape=(n_hidden_1, n_hidden_2))\n",
    "    b2 = pm.Normal('b2', mu=0, sigma=1, shape=(n_hidden_2,))\n",
    "\n",
    "    layer2 = pm.math.tanh(pm.math.dot(layer1, W2) + b2)\n",
    "\n",
    "    W3 = pm.Normal('W3', mu=0, sigma=1, shape=(n_hidden_2,))\n",
    "    b3 = pm.Normal('b3', mu=0, sigma=1)\n",
    "\n",
    "    mu = pm.math.dot(layer2, W3) + b3\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y_train_full)\n",
    "\n",
    "    approx_large = pm.fit(n=20000, method=\"advi\")\n",
    "    trace_bnn_large = approx_large.sample(draws=2000)\n",
    "print(az.summary(trace_bnn_large, var_names=['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'sigma']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large BNN Test MSE (trained on full data): 1.3101384420820532\n"
     ]
    }
   ],
   "source": [
    "post_mean_bnn_large = trace_bnn_large.posterior.mean(dim=[\"chain\", \"draw\"])\n",
    "\n",
    "W1_m = post_mean_bnn_large['W1'].values\n",
    "b1_m = post_mean_bnn_large['b1'].values\n",
    "W2_m = post_mean_bnn_large['W2'].values\n",
    "b2_m = post_mean_bnn_large['b2'].values\n",
    "W3_m = post_mean_bnn_large['W3'].values\n",
    "b3_m = float(post_mean_bnn_large['b3'].values)\n",
    "\n",
    "def forward_large_nn(X_):\n",
    "    layer1_ = np.tanh(X_ @ W1_m + b1_m)\n",
    "    layer2_ = np.tanh(layer1_ @ W2_m + b2_m)\n",
    "    return layer2_ @ W3_m + b3_m\n",
    "\n",
    "y_pred_large = forward_large_nn(X_test_full_scaled)\n",
    "mse_bnn_large = np.mean((y_pred_large - y_test_full)**2)\n",
    "print(\"Large BNN Test MSE (trained on full data):\", mse_bnn_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
